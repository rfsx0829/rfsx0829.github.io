---
title: 当今世界计算机硬件与软件发展现状
tags:
  - Computer
  - Science
date: 2019-05-13 14:41:13
categories: Papers
abbrlink: 513
---
# 当今世界计算机硬件与软件发展现状

## 简要

何为计算机？

计算机是一种根据一系列指令对数据进行处理的电子设备。

而不局限于我们经常接触到的个人电脑，个人电脑只是计算机的一种。

<!-- more -->

## 硬件

### 回首历史

#### 卡片机时代

最初的阶段是卡片时期，即使用**穿孔卡片**作为指令的载体。

1801年雅卡尔发明的提花织布机，可以通过相互串联的穿孔卡片达到半自动生产，且更换卡片无需更改机械设计，是可编程化机器的里程碑。

#### 数字管时期

1930年时，四则运算已经成为了桌上型机械计算器的基本功能，但是动力全靠机械，效率极低。

第一部全电子化的桌上计算器是英国人研发的，以**数字管**和177个**微型闸流管**来显示数字。

#### 电子电路和图灵机

1930年代后期到1940年代，受到二次大战影响，此一时期被认为是计算机发展史中的混乱时期，战争开启了现代电脑的时代，电子电路，继电器，电容及真空管相继登场，取代机械器件，数字计算器开始得到广泛发展，使用包含继电器或真空管的电路，以打孔纸带作为输入和主要（非短期）储存媒介。

1936年，图灵发表的研究报告对计算机和计算机科学领域造成巨大冲击，这篇报告主要是为了证明循环处理程式的死角，亦即停机问题的存在。图灵也以算法概念为通用计算机（纯理论器件）作出定义，即图灵机。

1937年，美国数学家兼工程师 克劳德·香农 在麻省理工学院发表他的硕士论文，是史上首度将布尔代数应用在电子继电器和电闸上的人。论文题为《中继和交换电路的符号分析》（A Symbolic Analysis of Relay and Switching Circuits），是数字电路设计的实践基础。

同年，在贝尔实验室工作的乔治组装出了一部以继电器表示二进位制的电脑模型机。

#### 冯·诺伊曼结构

冯·诺伊曼写过一篇广为流传的文章《EDVAC独家报告》（First Draft of a Report on the EDVAC），内容描述EDVAC将程式和计算中的资料，设计储存在同一内存内。冯·诺伊曼的这项设计后来被称为**冯·诺伊曼架构**，成为发展第一部真正具有运作弹性、一般用途数字电脑的设计基础。

第一部成功运作的**冯·诺伊曼结构**电脑是1948年曼彻斯特大学的小规模实验机，又称“宝贝”。随后在1949年，曼彻斯特马克一号电脑登场，功能完整，以威廉管和**磁鼓**作为内存媒介，并且引进**变址寄存器**的功能。

1954年，IBM推出一款电脑体积较小，价格和善，后来广受欢迎。这款IBM 650重达900公斤，附属的电力供应器件也有1350公斤左右，两者各安置在与人等高的橱柜里。原本其**磁鼓内存**只能保存2000个十位数字组，还需要晦涩难明的编程程序才能有效运作，诸如此类的内存限制在之后的十年间主宰了编程程序，直到编程模组一番革命性的改变后，软件开发才有了较人性化的转变。

### CPU发展历程

从四位处理器，到六十四位酷睿微架构，这其中又经历了什么呢

#### 摩尔定律

**摩尔定律**是由英特尔（Intel）创始人之一戈登·摩尔提出的。其内容为：**集成电路**上可容纳的**晶体管**数目，约每隔两年便会增加一倍；经常被引用的“18个月”，是由英特尔首席执行官大卫·豪斯提出：预计18个月会将芯片的性能提高一倍。

摩尔定律是简单评估半导体技术进展的经验法则，其重要的意义在于大抵而言，若在相同面积的晶圆下生产同样规格的IC，随着制程技术的进步，每隔一年半，IC产出量就可增加一倍，换算为成本，即每隔一年半成本可降低五成，平均每年成本可降低三成多。就摩尔定律延伸，IC技术每隔一年半推进一个世代。

但是摩尔定律已经快要失效了。

为了让摩尔定律延续到更小的器件尺度，学术界和工业界在不同的材料、器件结构和工作原理方面的探索一直在进行中。探索的问题之一是晶体管的闸极设计。随着器件尺寸越来越小，能否有效的控制晶体管中的电流变得越来越重要。相比于三面都有闸极的**多闸极晶体管**，**纳米线**晶体管将闸极四面围住，从而进一步改善了闸极对电流的控制。

随着新工艺节点的不断推出，晶体管中原子的数量已经越来越少，种种物理极限制约着其进一步发展。比如当闸极长度足够短的时候，**量子隧穿效应**就会发生，会导致漏电流增加。关于摩尔定律的终点究竟还有多远，看法并不一致。有预测认为摩尔定律的极限将在2025年左右到来，但也有更乐观的预测认为还能持续更久。

### 发展现状·集成芯片时期

第三波电脑世代来临，电脑使用度爆炸性的成长，这些全仰赖杰克·基尔比和罗伯特·诺伊斯的独立发明集成电路，引领英特尔发明微处理器。在1960年代，大量的电脑技术和过去的第二波电脑世代重叠，直到1975年后期，第二波电脑世代的机器仍在持续量产。

微处理器的诞生连带刺激微电脑的发展，轻便小巧，物廉价美的电脑成为个人及小公司唾手可得的工具，微电脑在1970年代初登场，到了1980年代后就已经成为家家户户都可看到的产品了。电脑逐渐成为微电脑架构的天下，再加上来自大型电脑的特色后，现如今已主宰大部分的电脑市场。

### 展望未来

#### 量子计算机

量子计算机，字面意思，就是使用量子逻辑计算的设备。用来储存数据的对象是量子比特，使用量子算法进行数据操作。

紧接着1985年大卫·杜斯提出了量子图灵机模型。人们研究量子计算机最初很重要的一个出发点是探索通用计算机的计算极限。当使用计算机模拟量子现象时，因为庞大的希尔伯特空间而数据量也变得庞大。一个完好的模拟所需的运算时间则变得相当长，甚至是不切实际的天文数字。理查德·费曼当时就想到如果用量子系统所构成的计算机来模拟量子现象则运算时间可大幅度减少，从而量子计算机的概念诞生。半导体靠控制集成电路来记录及运算信息，量子计算机则希望控制原子或小分子的状态，记录和运算信息。

## 软件

### 回首历史

在计算机刚被发明创造的时候，还是非常笨重和昂贵的，所以能用上的都不是一般人，运行的软件也基本都是大公司的专有软件，如金融，军事，科研等方面，帮忙统计和计算各种数据之类的。

继续跟随发展，计算机最开始不是靠软件盈利，几乎全靠硬件，随着硬件生产规模越来越大，硬件降价，之后，软件业从计算机工业中独立出来，成为一枝新秀。那时，硬件厂商开放了他们的系统软件（由于没有这种开放，不能进一步推销他们的产品），有几个公司开始提供定制软件的服务，然而那时没有专门经销软件的公司。由于软件免费，用户相互赠送，使软件不断重用与推广，并鼓励共享软件的发展。例如，IBM科学用户组织SHARE提供了软件重用的目录清单，包含了三角函数分类和合并计算的多个数学程序，并对进入了SHARE目录中的软件产品，能给予此领域的最高荣誉。

再以后，甚至因为不够稳定而曾经出现过软件危机。软件方面投入的资金和人力无法控制，软件开发完成的时间无法确定，软件的可靠性等等。

稳定阶段中开始出现了计算机学科的学术讨论。第一个计算机科学程序在六十年代晚期奠基，不久以后又编制了第一个管理信息系统程序。当时计算机软件工程还没有从计算机学科中分离出来，直到微型计算机年代才独立成为一门学科。尽管当时没有形成学科，然而软件工程研究一直在进行，重点聚集在计算机语言的问题上。

在此阶段，一些计算机科学家大力宣传他们引入的新思想。人工智能就是第一个竭力宣扬的学科，即称之为“有知觉”的机器，可以模仿人类大脑的功能，并期望代替人类大脑去做任何事情。

#### 互联网

最初的计算机 都只能单机工作，还不能互相联系，直到互联网的问世。

在1950年代，通信研究者认识到需要允许在不同计算机用户和通信网络之间进行常规的通信。这促使了分散网络、排队论和分组交换的研究。1960年，美国国防部高等研究计划署（ARPA）出于冷战考虑创建的ARPA网引发了技术进步并使其成为互联网发展的中心。1973年，ARPA网扩展成互联网。

经过多年，互联网成功地容纳了原有的计算机网络中的大多数（尽管像FidoNet的一些网络仍然保持独立）。这一快速发展要归功于互联网没有中央控制，以及互联网协议非私有的特质，前者造成了互联网有机的生长，而后者则鼓励了厂家之间的兼容，并防止了某一个公司在互联网上称霸。

### 发展现状

#### 人工智能

早期的人工智能研究人员直接模仿人类进行逐步的推理，就像是玩棋盘游戏或进行逻辑推理时人类的思考模式。到了1980和1990年代，利用**概率**和经济学上的概念，人工智能研究还发展了非常成功的方法处理**不确定**或不完整的资讯。

对于困难的问题，有可能需要大量的运算资源，也就是发生了“可能组合爆增”：当问题超过一定的规模时，计算机会需要天文数量级的存储器或是运算时间。寻找更有效的算法是优先的人工智能研究项目。

人类解决问题的模式通常是用最快捷、直观的判断，而不是有意识的、一步一步的推导，早期人工智能研究通常使用逐步推导的方式。人工智能研究已经于这种“次表征性的”解决问题方法获取进展：实体化Agent研究强调**感知运动**的重要性。神经网络研究试图以模拟人类和动物的大脑结构重现这种技能。

当前的人工智能只能算是初步发展，虽然已经影响到了我们生活的方方面面，但还是有很多可以做得更加好的地方，我近期还听到奋战在前线的优秀程序员说，众所周知人工智能靠人工，说明发展得还不够，还需快马加鞭。

#### 云计算

云计算是一种基于**互联网**的计算方式，通过这种方式，共享的软硬件资源和信息可以按需求提供给计算机各种终端和其他设备。

云计算是继1980年代大型计算机到客户端-服务器的大转变之后的又一种巨变。用户不再需要了解“云”中基础设施的细节，不必具有相应的专业知识，也无需直接进行控制。云计算描述了一种基于互联网的新的IT服务增加、使用和交付模式，通常涉及通过互联网来提供动态**易扩展**而且经常是**虚拟化**的资源。

云计算已经融入我们的生活了，很多东西都离不开云，即使有些人还不太了解。

云教育，云物联，等等，都与我们的息息相关，说个再贴近一点的，经常听到的阿里云腾讯云之类对外提供云服务的，前段时间还有同学找我帮助他申请域名和服务器的操作。

但当前的云计算的发展程度也还不够高，虽然同人工智能一样已经对人们的生活有了很大的提升，但他们的潜力远不止于此。

### 从编程方面来讲

#### 面向过程

或者也叫面向函数，在我最开始学习的时候，并不了解面向对象的代码架构设计和代码编程实现。所以要从字面上来区分函数和函数之间的关系，主要就靠函数命名、放在同一个代码文件里、放在同一个代码目录文件夹里来区分他们之间的关联性。

在函数时代，也没啥异常保护、异常处理、异常日志的函数编写基本原则，所以我们除了命名以外，主要注重的就是函数的输入数据参数以及格式、输出数据的参数以及格式。

#### 面向对象

因为有了类这个东西，所以函数就可以物以类聚了。有的函数属于私有函数只能这个类里面才能调用，有的函数属于公有函数可以供外部调用。

我们平时使用类还很初级，往往是一个源代码文件中就定义一个类。而且类也没有使用继承，也就是说我所有的类都是平行的，类和类之间通过Public型的公开函数调用才产生了关系。

#### 面向接口

何为面向接口？可能有很多人学了多年编程都没搞懂。

在系统分析和架构中，分清层次和依赖关系，每个层次不是直接向其上层提供服务（即不是直接实例化在上层中），而是通过定义一组接口，仅向上层暴露其接口功能，上层对于下层仅仅是接口依赖，而不依赖具体类。

这样做的好处是显而易见的，首先对系统灵活性大有好处。当下层需要改变时，只要接口及接口功能不变，则上层不用做任何修改。甚至可以在不改动上层代码时将下层整个替换掉，就像我们将一个金士顿16G硬盘换成一个闪迪的128G的硬盘，计算机其他地方不用做任何改动，而是把原硬盘拔下来、新硬盘插上就行了，因为计算机其他部分不依赖具体硬盘，而只依赖一个IDE接口，只要硬盘实现了这个接口，就可以替换上去。从这里看，程序中的接口和现实中的接口极为相似，所以我一直认为，接口（interface）这个词用的真是神似！

#### 面向组件

写C/S/S三层架构，业务逻辑层独立出来，而且是物理独立部署，这样客户端代码和业务逻辑层代码就必须要彻底分开，不能不清不楚地混杂在一起了。大量使用精心的接口设计、对象设计。

因为有了独立的业务逻辑层，那么这些代码（接口/类）何时创建对象实例，何时释放，这些对象实例要运行在哪个进程容器中，就有了要求了。因而就产生了组件容器和组件。组件容器来管理组件的全生命周期（安全、创建、并发访问控制、休眠、激活唤醒、计数、摧毁释放内存），组件管理器就来管理组件的注册、发现等。

正如《COM本质论》的作者Don Box说.NET就是更好的COM，对啊，微软的意思是，以后所有的应用都应该运行在组件容器中，不管是单机应用，还是C/S应用，还是C/S/S应用，每个应用都要运行在组件容器中，由组件容器来屏蔽和管理内存的创建与回收，不要把内存的创建与释放直接袒露给开发者，否则开发者技术能力水平不一，有的烂的程序员管理不好内存，很容易就会使应用占满内存并导致操作系统崩溃。

#### 微服务

**微服务**是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块为基础，利用模块化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关的 API 集相互通信。

随着发展，以前那一套很棒，很理想：一整套J2EE体系，WebService + EJB完美组件模型  +组件容器/管理器中间件，包括技术架构师。是多么多么大的市场啊。

但程序员是实用主义者，怎么简单怎么来。随着第二波互联网创业热崛起，大干快上才是王道，没钱用开源开干才是王道。且正值Google崛起如日中天，Google统一开放了自己的Open API，非常轻量。这已经是微服务流行的启动了。但这时候还不能称作微服务，可以称作简化服务。再加之大众需求趋势及亚马逊的推波助澜，终于到了微服务的大盛时期。

### 展望未来

#### 高度人工智能

前面讲了当前的人工智能发展，未来也是离不开人工智能的，在许多科幻电影中，人工智能几乎必不可少。人们也都憧憬着人工智能的发展。

经过60多年的发展，人工智能在算法、算力（计算能力）和算料（数据）等“三算”方面取得了重要突破，正处于从“不能用”到“可以用”的技术拐点，但是距离“很好用”还有诸多瓶颈。那么在可以预见的未来，人工智能发展将会出现怎样的趋势与特征呢？

**从专用智能向通用智能发展。**如何实现从专用人工智能向通用人工智能的跨越式发展，既是下一代人工智能发展的必然趋势，也是研究与应用领域的重大挑战。2016年10月，美国国家科学技术委员会发布《国家人工智能研究与发展战略计划》，提出在美国的人工智能中长期发展策略中要着重研究通用人工智能。阿尔法狗系统开发团队创始人戴密斯·哈萨比斯提出朝着“创造解决世界上一切问题的通用人工智能”这一目标前进。微软在2017年成立了通用人工智能实验室，众多感知、学习、推理、自然语言理解等方面的科学家参与其中。

**从人工智能向人机混合智能发展。**借鉴脑科学和认知科学的研究成果是人工智能的一个重要研究方向。人机混合智能旨在将人的作用或认知模型引入到人工智能系统中，提升人工智能系统的性能，使人工智能成为人类智能的自然延伸和拓展，通过人机协同更加高效地解决复杂问题。在我国新一代人工智能规划和美国脑计划中，人机混合智能都是重要的研发方向。

**从“人工+智能”向自主智能系统发展。**当前人工智能领域的大量研究集中在深度学习，但是深度学习的局限是需要大量人工干预，比如人工设计深度神经网络模型、人工设定应用场景、人工采集和标注大量训练数据、用户需要人工适配智能系统等，非常费时费力。因此，科研人员开始关注减少人工干预的自主智能方法，提高机器智能对环境的自主学习能力。例如阿尔法狗系统的后续版本阿尔法元从零开始，通过自我对弈强化学习实现围棋、国际象棋、日本将棋的“通用棋类人工智能”。在人工智能系统的自动化设计方面，2017年谷歌提出的自动化学习系统（AutoML）试图通过自动创建机器学习系统降低人员成本。

**人工智能将加速与其他学科领域交叉渗透。**人工智能本身是一门综合性的前沿学科和高度交叉的复合型学科，研究范畴广泛而又异常复杂，其发展需要与计算机科学、数学、认知科学、神经科学和社会科学等学科深度融合。随着超分辨率光学成像、光遗传学调控、透明脑、体细胞克隆等技术的突破，脑与认知科学的发展开启了新时代，能够大规模、更精细解析智力的神经环路基础和机制，人工智能将进入生物启发的智能阶段，依赖于生物学、脑科学、生命科学和心理学等学科的发现，将机理变为可计算的模型，同时人工智能也会促进脑科学、认知科学、生命科学甚至化学、物理、天文学等传统科学的发展。

**人工智能产业将蓬勃发展。**随着人工智能技术的进一步成熟以及政府和产业界投入的日益增长，人工智能应用的云端化将不断加速，全球人工智能产业规模在未来10年将进入高速增长期。例如，2016年9月，咨询公司埃森哲发布报告指出，人工智能技术的应用将为经济发展注入新动力，可在现有基础上将劳动生产率提高40%；到2035年，美、日、英、德、法等12个发达国家的年均经济增长率可以翻一番。2018年麦肯锡公司的研究报告预测，到2030年，约70%的公司将采用至少一种形式的人工智能，人工智能新增经济规模将达到13万亿美元。

#### 云计算时代

云计算还仅仅只是一个开始，相信再多些时日，必然是一个新的时代。或者说，其实现在已经是云时代的初期了。

所有的计算都在云上完成，只要有网络，就可以做成任何事情，云可以为任何终端提供几乎无穷的算力，和几乎无限的储存。再加上以后网络的高度普及，和高速通信的发展，云时代给人们生活带来的便利难以想象。

#### 去中心化

去中心化方面，现在比较成熟的技术非区块链莫属了，这是我最看好的技术之一，近两年很多人说区块链凉了，那群人是真的不懂技术，比特币是区块链的一个产物而已，比特币凉了跟区块链有什么关系，区块链技术永远不凉，去中心化是大势所趋，即使现在没有被人们所注意，在多年后，许多许多年后，必然是去中心化的天下。

集权永远不可取，民主第一！我相信任何热爱自由民主科学的人，都会热爱去中心化的技术！而这，才是未来！
